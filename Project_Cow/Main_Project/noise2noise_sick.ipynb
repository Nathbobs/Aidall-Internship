{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "base_path = os.path.abspath(\"..\")\n",
    "main_project_path = os.path.join(base_path, 'Main_Project')\n",
    "raw_sick_cow_path = os.path.join(main_project_path, 'RAW_Sick_Cow')\n",
    "noise_path = os.path.join(main_project_path, 'Noise')\n",
    "output_path = os.path.join(main_project_path, 'noise2noise_Denoised_Sick_Cow')\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zb/pmsl1zpx7sg1hqhm44bmnf9w0000gn/T/ipykernel_32671/218431003.py:9: RuntimeWarning: invalid value encountered in divide\n",
      "  return (audio - np.min(audio)) / (np.max(audio) - np.min(audio))\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "sr = 22050  # Sample rate for audio\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "max_len = 10 * sr  # Example: 10 seconds of audio\n",
    "\n",
    "# Data normalization function\n",
    "def normalize_audio(audio):\n",
    "    return (audio - np.min(audio)) / (np.max(audio) - np.min(audio))\n",
    "\n",
    "\n",
    "# 1. Data Loading and Preprocessing\n",
    "\n",
    "def load_wav_files(path):\n",
    "    files = []\n",
    "    for file_name in os.listdir(path):\n",
    "        if file_name.endswith('.wav'):\n",
    "            wav_path = os.path.join(path, file_name)\n",
    "            wav_data, _ = librosa.load(wav_path, sr=sr)\n",
    "            files.append(wav_data)\n",
    "    return files\n",
    "\n",
    "# Padding or truncating function\n",
    "def pad_or_truncate(audio, max_len):\n",
    "    if len(audio) > max_len:\n",
    "        return audio[:max_len]\n",
    "    elif len(audio) < max_len:\n",
    "        return np.pad(audio, (0, max_len - len(audio)), mode='constant')\n",
    "    else:\n",
    "        return audio\n",
    "\n",
    "# Load and preprocess all data\n",
    "raw_sick_cow_files = load_wav_files(raw_sick_cow_path)\n",
    "noise_files = load_wav_files(noise_path)\n",
    "\n",
    "# Apply padding/truncating and normalization\n",
    "raw_sick_cow_data = np.array([pad_or_truncate(normalize_audio(audio), max_len) for audio in raw_sick_cow_files])\n",
    "noise_data = np.array([pad_or_truncate(normalize_audio(audio), max_len) for audio in noise_files])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 2. Model Definition\n",
    "\n",
    "def build_noise2noise_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv1D(64, 9, padding='same', input_shape=input_shape, activation='relu'))\n",
    "    model.add(layers.Conv1D(64, 9, padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "    model.add(layers.Conv1D(128, 9, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv1D(128, 9, padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "    model.add(layers.Conv1D(256, 9, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv1D(256, 9, padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(2))\n",
    "\n",
    "    model.add(layers.Conv1DTranspose(256, 9, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv1DTranspose(256, 9, padding='same', activation='relu'))\n",
    "    model.add(layers.UpSampling1D(2))\n",
    "\n",
    "    model.add(layers.Conv1DTranspose(128, 9, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv1DTranspose(128, 9, padding='same', activation='relu'))\n",
    "    model.add(layers.UpSampling1D(2))\n",
    "\n",
    "    model.add(layers.Conv1DTranspose(64, 9, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv1DTranspose(64, 9, padding='same', activation='relu'))\n",
    "    model.add(layers.UpSampling1D(2))\n",
    "\n",
    "    model.add(layers.Conv1D(1, 9, padding='same', activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "input_shape = (None, 1)  # (time_steps, channels)\n",
    "model = build_noise2noise_model(input_shape)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 114ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n",
      "Denoising complete. Denoised files saved in /Users/nathbobs/Documents/Aidall Internship/Project_Cow/Main_Project/noise2noise_Denoised_Sick_Cow\n"
     ]
    }
   ],
   "source": [
    "# 4. Denoising and Saving Output\n",
    "\n",
    "def denoise_audio(model, noisy_audio):\n",
    "    noisy_audio = noisy_audio[np.newaxis, ..., np.newaxis]  # Reshape for model input\n",
    "    denoised_audio = model.predict(noisy_audio)\n",
    "    return denoised_audio[0, :, 0]  # Reshape back to 1D\n",
    "\n",
    "# Process and save each file\n",
    "for i, file_name in enumerate(os.listdir(raw_sick_cow_path)):\n",
    "    if file_name.endswith('.wav'):\n",
    "        noisy_wav_path = os.path.join(raw_sick_cow_path, file_name)\n",
    "        noisy_audio, _ = librosa.load(noisy_wav_path, sr=sr)\n",
    "        \n",
    "        denoised_audio = denoise_audio(model, noisy_audio)\n",
    "        denoised_audio = denoised_audio * 32767  # Revert normalization if necessary\n",
    "        denoised_audio = denoised_audio.astype(np.int16)\n",
    "        \n",
    "        output_wav_path = os.path.join(output_path, file_name)\n",
    "        sf.write(output_wav_path, denoised_audio, sr)\n",
    "\n",
    "print(\"Denoising complete. Denoised files saved in\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
